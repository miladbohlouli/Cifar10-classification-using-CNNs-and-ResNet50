{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CIFAR10 using CNN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AH-aGI2r8AiA","colab_type":"text"},"source":["<h1> CIFAR10 classification using CNNs</h1>\n","\n","In this Project we attempt to implement a CNN classfier for STL dataset using Tensorflow Estimators. The model code is defined in a way that it can be changed easily. The goal of the next cell is to copy the dataset from Google Drive to the remote Disk and do the computations. "]},{"cell_type":"markdown","metadata":{"id":"B-ffwafIss9a","colab_type":"text"},"source":["<h3> Dataset evaluation</h3>\n","This cell simply tries to check if the importing process of dataset has worked well and outputs the shape of one of the samples in dataset. So run this if you just wanna check the importing process otherwise do not tend to run this cause all the dataset will be loaded in RAM, thus in the next cells as we do have some other imports, you may get an memory error."]},{"cell_type":"code","metadata":{"id":"Fn_HoQ3T3l49","colab_type":"code","outputId":"9260ae9f-2535-405f-deea-03dcb87ff690","executionInfo":{"status":"ok","timestamp":1560198470590,"user_tz":-270,"elapsed":4495,"user":{"displayName":"milad bohlouli","photoUrl":"https://lh6.googleusercontent.com/-TCzCtB0Ck24/AAAAAAAAAAI/AAAAAAAAECM/zctIG7ASKTM/s64/photo.jpg","userId":"03835783478152993975"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from matplotlib import pyplot as plt\n","import tensorflow as tf\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import time\n","import sklearn\n","from google.colab import drive\n","\n","drive.mount('content/')\n","\n","img_size = 32\n","img_flat_size = 32 * 32\n","learning_rate = 0.0001\n","batch_size = 64\n","local_direc = \"./model\"\n","drive_direc = \"./content/My Drive/Projects/Colab/NN/HW4/part5/model\"\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at content/; to attempt to forcibly remount, call drive.mount(\"content/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PKs9vYrOweip","colab_type":"text"},"source":["<h3>Model and input functions definition</h3>\n","In the next cell a dynamic model of the CNNs has been implemented and the dataset will be imported as well. "]},{"cell_type":"code","metadata":{"id":"KSLodsycWiZZ","colab_type":"code","outputId":"548a2769-847c-430b-a235-3222df8a0ccf","executionInfo":{"status":"error","timestamp":1560198492559,"user_tz":-270,"elapsed":19468,"user":{"displayName":"milad bohlouli","photoUrl":"https://lh6.googleusercontent.com/-TCzCtB0Ck24/AAAAAAAAAAI/AAAAAAAAECM/zctIG7ASKTM/s64/photo.jpg","userId":"03835783478152993975"}},"colab":{"base_uri":"https://localhost:8080/","height":1556}},"source":["def eval_conf_matrix(labels, predictions, params):\n","\n","    num_classes = params['num_classes']\n","    conf_matrix = tf.confusion_matrix(labels, predictions, num_classes=num_classes)\n","    conf_matrix_sum = tf.Variable(tf.zeros(shape = (num_classes, num_classes), dtype=tf.int32),\n","                                  name=\"confusion_matrix\",\n","                                  trainable=False,\n","                                  collections=[tf.GraphKeys.LOCAL_VARIABLES])\n","\n","    update_op = tf.assign_add(conf_matrix_sum, conf_matrix)\n","    return tf.convert_to_tensor(conf_matrix_sum), update_op\n","\n","  \n","  # This is the part where we definr the model\n","def model_fn(features, labels, mode, params):\n","    \n","  global_step = tf.train.get_global_step()\n","    \n","  # The input layer\n","  input_layer = tf.reshape(features, [-1, img_size, img_size, 3])\n","  \n","  ##################################################################\n","  # This part is the implementation of the first layer of\n","  #   convolution and subsampling.\n","  ##################################################################\n","  # Convolution layer\n","  conv = tf.layers.conv2d(\n","        inputs=input_layer,\n","        filters=params['conv_layers'][0]['filters'],\n","        kernel_size=params['conv_layers'][0]['kernel_size'], \n","        padding=\"same\",\n","        activation=tf.nn.relu)\n","\n","\n","  # Pooling layer\n","  pool = tf.layers.max_pooling2d(\n","        inputs=conv, \n","        pool_size=params['conv_layers'][0]['pooling_size'], \n","        strides=params['conv_layers'][0]['strides'])\n","\n","  ##################################################################\n","  # This part is for dynamicness of the model. The structure of the\n","  #  model will be imported in params parameter and using that \n","  #  the NN model will be implemented, thus there is no need to change\n","  #  the model_fn function if you want to change the structure.\n","  ##################################################################  \n","  for conv_pool_layer in params['conv_layers'][1:]:\n","    \n","    # Interior Convolutional layer \n","    conv = tf.layers.conv2d(\n","        inputs=pool,\n","        filters=conv_pool_layer['filters'],\n","        kernel_size=conv_pool_layer['kernel_size'], \n","        padding=\"same\",\n","        activation=tf.nn.relu)\n","\n","\n","    # Interior Pooling layer \n","    pool = tf.layers.max_pooling2d(\n","        inputs=conv, \n","        pool_size=conv_pool_layer['pooling_size'], \n","        strides=conv_pool_layer['strides'])\n","\n","  \n","  ##################################################################\n","  # In this part we implement the dense layers. These layers are \n","  #  dynamic as well, thus there is no need to change this part if \n","  #  structure reformation is required. Just conduct the changes\n","  #  in params parameter passed to this method.\n","  ##################################################################  \n","  \n","  # Size of the output of the last pooling layer to feed to the dense layers\n","  new_size = int(img_size / (2 ** len(params['conv_layers'])))\n","  \n","  # raveled form of the outputs of last pooling layer\n","  flat_output = tf.reshape(pool, [-1, new_size * new_size * params['conv_layers'][-1]['filters']])\n","\n","  \n","  # First dense layer\n","  dense = tf.layers.dense(inputs=flat_output, \n","                          units=params['dense_layers'][0]['units'], \n","                          activation=tf.nn.relu)\n","  \n","  # dropout applied to the dense layers\n","  dropout = tf.layers.dropout(inputs=dense, rate=params['dense_layers'][0]['dropout'])\n","\n","  for dense_layer in params['dense_layers'][1:]:\n","    \n","    # interior dense layers\n","    dense = tf.layers.dense(inputs=dropout, \n","                          units=dense_layer['units'], \n","                          activation=tf.nn.relu)\n","    \n","    \n","    # If dropout is not necessary, set the rate to zero in params dictionary\n","    dropout = tf.layers.dropout(inputs=dense, rate=dense_layer['dropout'])\n","\n","  # The logits will be calculated from the last dropout layer\n","  logits = tf.layers.dense(inputs=dropout, units=params['num_classes'])\n","  \n","  prediction_labels = tf.argmax(logits, 1)\n","  probabilities = tf.nn.softmax(logits)\n","\n","\n","  predictions = {\n","      'prediction_labels':prediction_labels,\n","      'probabilities': probabilities\n","  }\n","\n","  #   This part is for the prediction part of the model\n","  if mode == tf.estimator.ModeKeys.PREDICT:\n","      return tf.estimator.EstimatorSpec(\n","          mode = mode,\n","          predictions = predictions\n","      )\n","\n","  accuracy = tf.metrics.accuracy(labels, prediction_labels)\n","  confusion_matrix_tuple = eval_conf_matrix(labels, prediction_labels, params)\n","\n","  cross_entropy = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n","\n","    # This is the evaluation part of the model\n","  if mode == tf.estimator.ModeKeys.EVAL:\n","      tf.summary.scalar(\"Evaluation_accuracy\", accuracy[1])\n","      return tf.estimator.EstimatorSpec(\n","          mode=mode,\n","          loss=cross_entropy,\n","          eval_metric_ops={'eval_accuracy': accuracy,\n","                           'confusion_matrix': confusion_matrix_tuple},\n","          predictions = predictions,\n","          evaluation_hooks=None\n","      )\n","\n","  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","  train_op = optimizer.minimize(cross_entropy, global_step=global_step)\n","\n","  train_hook_list = []\n","  train_tensor_logs = {\n","      'accuracy':accuracy[1],\n","      'loss':cross_entropy,\n","      'global_step':global_step\n","  }\n","\n","  train_hook_list.append(tf.train.LoggingTensorHook(\n","      tensors=train_tensor_logs, every_n_iter=100\n","  ))\n","\n","  #   This is the training part of the model\n","  if mode == tf.estimator.ModeKeys.TRAIN:\n","      tf.summary.scalar(\"Training_accuracy\", accuracy[1])\n","      return tf.estimator.EstimatorSpec(\n","          mode=mode,\n","          loss=cross_entropy,\n","          train_op=train_op,\n","          eval_metric_ops=None,\n","          training_hooks=train_hook_list\n","      )\n","\n","\n","def STL_classifier(_):\n","  \n","  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","  \n","  x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, random_state=10)\n","  \n","  y_train = np.asarray(y_train, dtype=np.int64)\n","  y_val = np.asarray(y_val, dtype=np.int64)\n","  y_test = np.asarray(y_test, dtype=np.int64)\n","  x_train = np.asarray(x_train, dtype=np.float64)\n","  x_test = np.asarray(x_test, dtype=np.float64)\n","  x_val = np.asarray(x_val, dtype=np.float64)\n","\n","  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n","      x=x_train,\n","      y=y_train,\n","      batch_size=batch_size,\n","      num_epochs=7,\n","      shuffle=True)\n","\n","  eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n","      x=x_val,\n","      y=y_val,\n","      batch_size=batch_size,\n","      num_epochs=1,\n","      shuffle=False)\n","\n","  test_input_fn = tf.estimator.inputs.numpy_input_fn(\n","      x=x_test,\n","      y=y_test,\n","      batch_size=batch_size,\n","      num_epochs=1,\n","      shuffle=False)\n","\n","  saving_configuration = tf.estimator.RunConfig(save_checkpoints_secs=300, keep_checkpoint_max=2)\n","  \n","  image_classifier = tf.estimator.Estimator(\n","      model_dir=drive_direc,\n","      model_fn=model_fn,\n","      config=saving_configuration,\n","      params={\n","          'num_classes':10,\n","          'conv_layers': [{'kernel_size':[5, 5], 'filters':32, 'pooling_size':[2, 2], 'strides':2},\n","                          {'kernel_size':[5, 5], 'filters':32, 'pooling_size':[2, 2], 'strides':2}],\n","          'dense_layers': [{'units': 512, 'dropout': 0.4}]\n","      })\n","\n","  start = time.time()\n","\n","  for i in range(0):\n","      image_classifier.train(input_fn=train_input_fn, steps = None)\n","      metrices = image_classifier.evaluate(input_fn=eval_input_fn)\n","      print(\"\\n\\n******************************************************\\nConfusion matrix for evaluation data:\\n\\n\"\n","            + str(metrices['confusion_matrix']) + \"\\n\\n\\nOther evaluation matrices:\"\n","            + \"\\nEvaluation accuracy:%.2f\\tLoss:%.2f\\tGlobal step:%d\"%(metrices['eval_accuracy'] * 100, metrices['loss'], metrices['global_step'])\n","            + \"\\n******************************************************\")\n","\n","  end = time.time()\n","  print(\"******************************************************\\nTraining time:%.4s seconds\"%str(end - start)\n","        + \"\\n******************************************************\\n\\n\")\n","\n","  predictions = image_classifier.predict(\n","      input_fn=test_input_fn,\n","      yield_single_examples=False\n","  )\n","\n","  accuracy = 0\n","  i = 0\n","  conf_matrix = np.zeros((10, 10))\n","  for epoch_result in predictions:\n","      accuracy += sklearn.metrics.accuracy_score(y_test[i * batch_size: (i + 1) * batch_size], \n","                                                 epoch_result['prediction_labels'])\n","\n","      \n","      conf_matrix += sklearn.metrics.confusion_matrix(y_test[i * batch_size: (i + 1) * batch_size], \n","                                                      epoch_result['prediction_labels'], labels=range(10))\n","      \n","      i = i + 1\n","  print(i)\n","  print(\"\\n******************************************************\\nConfusion matrix for test data:\\n\\n\" + str(conf_matrix)\n","        + \"\\n******************************************************\")\n","\n","  print(conf_matrix)\n","  \n","  print(\"\\n******************************************************\\nAccuracy for test data: %.2f\"\n","        \"\\n******************************************************\\n\"%(accuracy / i * 100))\n","\n","if __name__ == '__main__':\n","  tf.app.run(STL_classifier)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 4s 0us/step\n","INFO:tensorflow:Using config: {'_model_dir': './content/My Drive/Projects/Colab/NN/HW4/part5/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 300, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 2, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb70dbd8ef0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","******************************************************\n","Training time:2.38 seconds\n","******************************************************\n","\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","INFO:tensorflow:Calling model_fn.\n","WARNING:tensorflow:From <ipython-input-3-12339a48a135>:32: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.conv2d instead.\n","WARNING:tensorflow:From <ipython-input-3-12339a48a135>:39: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.max_pooling2d instead.\n","WARNING:tensorflow:From <ipython-input-3-12339a48a135>:82: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n","WARNING:tensorflow:From <ipython-input-3-12339a48a135>:85: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dropout instead.\n","INFO:tensorflow:Done calling model_fn.\n","INFO:tensorflow:Graph was finalized.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","INFO:tensorflow:Restoring parameters from ./content/My Drive/Projects/Colab/NN/HW4/part5/model/model.ckpt-32816\n","INFO:tensorflow:Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py:809: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","157\n","\n","******************************************************\n","Confusion matrix for test data:\n","\n","[[658.  18.  52.  15.  33.   6.  19.  30. 114.  55.]\n"," [ 35. 641.   5.  18.   8.   8.  15.  14.  75. 181.]\n"," [ 87.  11. 412.  64. 145.  81.  81.  73.  26.  20.]\n"," [ 29.  18.  62. 350.  97. 183. 106.  81.  32.  42.]\n"," [ 31.   8.  87.  59. 528.  56.  80. 119.  19.  13.]\n"," [ 18.  13.  62. 160.  76. 478.  51.  93.  31.  18.]\n"," [ 16.  14.  50.  53.  74.  42. 672.  30.  18.  31.]\n"," [ 18.  10.  27.  46.  74.  74.  19. 706.   8.  18.]\n"," [ 85.  36.  19.  13.  11.   7.  13.   9. 766.  41.]\n"," [ 43.  95.   7.  16.  11.  12.  16.  39.  62. 699.]]\n","******************************************************\n","[[658.  18.  52.  15.  33.   6.  19.  30. 114.  55.]\n"," [ 35. 641.   5.  18.   8.   8.  15.  14.  75. 181.]\n"," [ 87.  11. 412.  64. 145.  81.  81.  73.  26.  20.]\n"," [ 29.  18.  62. 350.  97. 183. 106.  81.  32.  42.]\n"," [ 31.   8.  87.  59. 528.  56.  80. 119.  19.  13.]\n"," [ 18.  13.  62. 160.  76. 478.  51.  93.  31.  18.]\n"," [ 16.  14.  50.  53.  74.  42. 672.  30.  18.  31.]\n"," [ 18.  10.  27.  46.  74.  74.  19. 706.   8.  18.]\n"," [ 85.  36.  19.  13.  11.   7.  13.   9. 766.  41.]\n"," [ 43.  95.   7.  16.  11.  12.  16.  39.  62. 699.]]\n","\n","******************************************************\n","Accuracy for test data: 59.03\n","******************************************************\n","\n"],"name":"stdout"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"FajTo4gE5ai5","colab_type":"code","colab":{}},"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip\n","\n","!rm -rd model\n","!mkdir model\n","!cp -rd ./content/My\\ Drive/Projects/Colab/NN/HW4/part5/model/* ./model/\n","\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(\"./model\")\n",")\n","\n","get_ipython().system_raw('./ngrok http 6006 &')\n","\n","! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[]}]}